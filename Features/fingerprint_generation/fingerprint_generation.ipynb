{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install dependencies\n",
    "#%pip install pandas numpy molfeat datamol rdkit ankh torch peptides sklearn lightgbm tmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: unsuccessful initial attempt using frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: unsuccessful initial attempt using frozen solve. Retrying with flexible solve.\n",
      "Solving environment: - \n",
      "Found conflicts! Looking for incompatible packages.\n",
      "This can take several minutes.  Press CTRL-C to abort.\n",
      "Examining conflict for wheel pyparsing libnghttp2 sympy pillow joblib python_abi- \\ - | ^C\n",
      "                                                                               failed\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%conda install -c tmap tmap \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7861328125\n",
      "0.7861328125\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "import tmap as tm\n",
    "from map4 import MAP4Calculator\n",
    "\n",
    "dim = 1024\n",
    "\n",
    "MAP4 = MAP4Calculator(dimensions=dim)\n",
    "ENC = tm.Minhash(dim)\n",
    "\n",
    "smiles_a = 'c1ccccc1'\n",
    "mol_a = Chem.MolFromSmiles(smiles_a)\n",
    "map4_a = MAP4.calculate(mol_a)\n",
    "\n",
    "\n",
    "smiles_b = 'c1cccc(N)c1'\n",
    "mol_b = Chem.MolFromSmiles(smiles_b)\n",
    "map4_b = MAP4.calculate(mol_b)\n",
    "\n",
    "# or use parallelized version:\n",
    "fps = MAP4.calculate_many([mol_a, mol_b])\n",
    "\n",
    "\n",
    "print(ENC.get_distance(map4_a, map4_b))\n",
    "\n",
    "print(ENC.get_distance(fps[0], fps[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n",
      "2023-09-17 17:57:25.562456: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-17 17:57:47.569381: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/intel/compilers_and_libraries_2018.3.222/linux/mpi/intel64/lib:/opt/intel/compilers_and_libraries_2018.3.222/linux/mpi/mic/lib:/opt/intel/compilers_and_libraries_2018.3.222/linux/mpi/intel64/lib:/opt/intel/compilers_and_libraries_2018.3.222/linux/mpi/mic/lib::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/anaconda/envs/azureml_py38/lib/:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/anaconda/envs/azureml_py38/lib/:/anaconda/envs/azureml_py38/lib/\n",
      "2023-09-17 17:57:47.574699: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/intel/compilers_and_libraries_2018.3.222/linux/mpi/intel64/lib:/opt/intel/compilers_and_libraries_2018.3.222/linux/mpi/mic/lib:/opt/intel/compilers_and_libraries_2018.3.222/linux/mpi/intel64/lib:/opt/intel/compilers_and_libraries_2018.3.222/linux/mpi/mic/lib::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/anaconda/envs/azureml_py38/lib/:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/anaconda/envs/azureml_py38/lib/:/anaconda/envs/azureml_py38/lib/\n",
      "2023-09-17 17:57:47.574722: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "#import libraries + load dataset\n",
    "import pandas as pd\n",
    "import datamol as dm\n",
    "from molfeat.calc import FPCalculator\n",
    "from molfeat.trans import MoleculeTransformer\n",
    "import numpy as np\n",
    "import os\n",
    "import ankh\n",
    "import torch\n",
    "import peptides\n",
    "\n",
    "#df = pd.read_pickle('../../Data/processed/clean_df.pkl')\n",
    "\n",
    "df = pd.read_pickle('../../Data/processed/clean_df_grouped.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Molecular Fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to Sequence to SMILES\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import AllChem\n",
    "def seq_to_smiles(seq):\n",
    "    mol = Chem.MolFromFASTA(seq)\n",
    "    smiles = Chem.MolToSmiles(mol)\n",
    "    return smiles\n",
    "\n",
    "df['SMILES'] = df['Sequence'].apply(seq_to_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['maccs', 'avalon', 'ecfp', 'fcfp', 'topological', 'atompair', 'rdkit', 'pattern', 'layered', 'map4', 'secfp', 'erg', 'estate', 'avalon-count', 'rdkit-count', 'ecfp-count', 'fcfp-count', 'topological-count', 'atompair-count'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print all available molecular fingerprints from the molfeat library\n",
    "\n",
    "from molfeat.calc import FP_FUNCS\n",
    "FP_FUNCS.keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "fingerprints = {}\n",
    "#load data\n",
    "with open('../Fingerprints/fingerprints_df_grouped.pickle', 'rb') as f:\n",
    "    fingerprints = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['map4', 'erg', 'atompair-count', 'ecfp', 'layered', 'topological', 'rdkit', 'binary profile of physicochemical property', 'one-hot-encoded-sequence', 'peptide_descriptors', 'ankh_base_embedding', 'mean_embeddings_large', 'maccs', 'avalon', 'fcfp', 'atompair', 'pattern', 'secfp', 'estate', 'avalon-count', 'rdkit-count', 'ecfp-count', 'fcfp-count', 'topological-count'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fingerprints.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating:  avalon\n",
      "generated:  avalon 1230\n",
      "generating:  fcfp\n",
      "generated:  fcfp 1230\n",
      "generating:  atompair\n",
      "generated:  atompair 1230\n",
      "generating:  pattern\n",
      "generated:  pattern 1230\n",
      "generating:  secfp\n",
      "generated:  secfp 1230\n",
      "generating:  estate\n",
      "generated:  estate 1230\n",
      "generating:  avalon-count\n",
      "generated:  avalon-count 1230\n",
      "generating:  rdkit-count\n",
      "generated:  rdkit-count 1230\n",
      "generating:  ecfp-count\n",
      "generated:  ecfp-count 1230\n",
      "generating:  fcfp-count\n",
      "generated:  fcfp-count 1230\n",
      "generating:  topological-count\n",
      "generated:  topological-count 1230\n"
     ]
    }
   ],
   "source": [
    "#generate the fingerprints\n",
    "#selection = [\"pattern\", \"ecfp\", \"maccs\", \"atompair\", \"rdkit\", \"topological\", \"atompair-count\"]\n",
    "selection = ['maccs', 'avalon', 'ecfp', 'fcfp', 'topological', 'atompair', 'rdkit', 'pattern', 'layered', 'map4', 'secfp', 'erg', 'estate', 'avalon-count', 'rdkit-count', 'ecfp-count', 'fcfp-count', 'topological-count', 'atompair-count']\n",
    "#selection = [\"map4\", \"erg\", \"atompair-count\", \"ecfp\", \"layered\", \"topological\", \"rdkit\"]\n",
    "for fingerprint in selection:\n",
    "    if fingerprint in fingerprints:\n",
    "        continue\n",
    "    print('generating: ', fingerprint)\n",
    "    calc = FPCalculator(fingerprint)\n",
    "    trans = MoleculeTransformer(calc)\n",
    "    with dm.without_rdkit_log():\n",
    "        fingerprints[fingerprint] = trans.transform(df.SMILES.values)\n",
    "    print('generated: ', fingerprint, len(fingerprints[fingerprint]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Based Fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" From the Pfeature library (physicochemical properties:\n",
    "\n",
    "This method generates the output as a binary profile for each sequence, which explains if a particular physicochemical property is present in a sequence. \n",
    "A single residue is represented by a vector of length 25, where each value is corresponding to a particular physicochemical property, \n",
    "if a particular residue is having the property then that position will be assigned as 1 else 0. \n",
    "Hence, if a sequence is given with length L, the output vector will be of size 25*L.\n",
    "\"\"\"\n",
    "\n",
    "profiles = {\n",
    "    \"A\": [0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0],\n",
    "    \"C\": [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0],\n",
    "    \"D\": [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0],\n",
    "    \"E\": [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1],\n",
    "    \"F\": [0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1],\n",
    "    \"G\": [0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0],\n",
    "    \"H\": [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1],\n",
    "    \"I\": [0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1],\n",
    "    \"K\": [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1],\n",
    "    \"L\": [0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1],\n",
    "    \"M\": [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1],\n",
    "    \"N\": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0],\n",
    "    \"P\": [0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0],\n",
    "    \"Q\": [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1],\n",
    "    \"R\": [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "    \"S\": [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0],\n",
    "    \"T\": [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0],\n",
    "    \"V\": [0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0],\n",
    "    \"W\": [0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1],\n",
    "    \"Y\": [0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
    "}\n",
    "\n",
    "sequences = df['Sequence'].values\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "# Pad the binary profiles with zeros to make them all have the same length\n",
    "padded_profiles = [\n",
    "    [profiles[aa] for aa in seq]\n",
    "    for seq in sequences\n",
    "]\n",
    "padded_profiles = [\n",
    "    seq + [[0] * 25] * (max_sequence_length - len(seq))\n",
    "    for seq in padded_profiles\n",
    "]\n",
    "\n",
    "padded_profiles = np.array(padded_profiles)\n",
    "padded_profiles = padded_profiles.reshape(padded_profiles.shape[0], -1)\n",
    "\n",
    "fingerprints['binary profile of physicochemical property'] = padded_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate one-hot encoding fingerprint\n",
    "\n",
    "amino_acids = [\"A\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"K\", \"L\", \"M\", \"N\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"V\", \"W\", \"Y\"]\n",
    "\n",
    "# Create a dictionary of one-hot vectors for each amino acid\n",
    "profiles = {aa: [1 if i == idx else 0 for i in range(len(amino_acids))] for idx, aa in enumerate(amino_acids)}\n",
    "\n",
    "sequences = df['Sequence'].values\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "# Pad the binary profiles with zeros to make them all have the same length\n",
    "padded_profiles = [\n",
    "    [profiles[aa] for aa in seq]\n",
    "    for seq in sequences\n",
    "]\n",
    "\n",
    "padded_profiles = [\n",
    "    seq + [[0] * len(amino_acids)] * (max_sequence_length - len(seq))\n",
    "    for seq in padded_profiles\n",
    "]\n",
    "\n",
    "padded_profiles = np.array(padded_profiles)\n",
    "padded_profiles = padded_profiles.reshape(padded_profiles.shape[0], -1)\n",
    "\n",
    "fingerprints['one-hot-encoded-sequence'] = padded_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peptide_descriptors from the Peptides python library\n",
    "sequences = df['Sequence'].values\n",
    "descriptors = pd.DataFrame([ peptides.Peptide(s).descriptors() for s in sequences ])\n",
    "descriptors = descriptors.to_numpy()\n",
    "fingerprints['peptide_descriptors'] = descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Embedding Fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b30266b240614172a730c15f98d9deec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.82k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be0784f3a3554e95a2d4b1fadec25656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/31.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a8532311fd46f49c99a986177692fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1981e13c90d457bb1c321cfba2a5d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/826 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd5077cf13cb4bf2a0d4ae52162fa804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1230, 40, 768])\n"
     ]
    }
   ],
   "source": [
    "# ankh base model large feature vector\n",
    "model, tokenizer = ankh.load_base_model()\n",
    "model.eval()\n",
    "\n",
    "sequences = [list(seq) for seq in df['Sequence']]\n",
    "\n",
    "outputs = tokenizer.batch_encode_plus(sequences, \n",
    "                                  add_special_tokens=True, \n",
    "                                  padding=True, \n",
    "                                  is_split_into_words=True, \n",
    "                                  return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "  embeddings = model(input_ids=outputs['input_ids'], attention_mask=outputs['attention_mask'])\n",
    "\n",
    "print(embeddings[0].shape)\n",
    "\n",
    "array = np.array(embeddings[0])\n",
    "\n",
    "reshaped_array = array.reshape(embeddings[0].shape[0], -1)\n",
    "\n",
    "fingerprints['ankh_base_embedding'] = reshaped_array.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c78429f2f1428a81d35474527fa5fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.85k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6ee54e59d8245feaf201012ab93d5dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/31.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9285c8ed764405cbd9d260f682ac226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d5661ca9c984e65b489f3e84461b2b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/849 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a158b995fd1741dc986bb7f8c1592dd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/7.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1230, 1536) mean_embeddings_large.shape\n"
     ]
    }
   ],
   "source": [
    "# ankh large model small feature vector (because averaged instead of appended)\n",
    "model, tokenizer = ankh.load_large_model()\n",
    "model.eval()\n",
    "\n",
    "sequences = [list(seq) for seq in df['Sequence']]\n",
    "\n",
    "outputs = tokenizer.batch_encode_plus(sequences, \n",
    "                                  add_special_tokens=True, \n",
    "                                  padding=True, \n",
    "                                  is_split_into_words=True, \n",
    "                                  return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "  embeddings = model(input_ids=outputs['input_ids'], attention_mask=outputs['attention_mask'])\n",
    "\n",
    "mean_embeddings_large = []\n",
    "for embedding in embeddings[0]:\n",
    "    embedding = np.array(embedding)\n",
    "    mean_embeddings_large.append(np.mean(embedding, axis=0))\n",
    "\n",
    "mean_embeddings_large = np.array(mean_embeddings_large)\n",
    "print(mean_embeddings_large.shape, \"mean_embeddings_large.shape\")\n",
    "\n",
    "fingerprints['mean_embeddings_large'] = mean_embeddings_large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write fingerprints to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the fingerprint dictionary\n",
    "import pickle\n",
    "with open(\"../Fingerprints/fingerprints_df_grouped.pickle\", \"wb\") as f:\n",
    "    pickle.dump(fingerprints, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
